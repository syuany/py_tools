import os
import sys
import xxhash
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Manager, Lock

def get_fast_hash(filename):
    """å¿«é€Ÿå“ˆå¸Œï¼ˆå¤´å°¾æŠ½æ ·ï¼‰"""
    hasher = xxhash.xxh64()
    try:
        with open(filename, 'rb') as f:
            # è¯»å–å¤´16KB
            head = f.read(16384)
            hasher.update(head)
            
            # è¯»æ–‡ä»¶å¤§å°å¹¶å†³å®šæ˜¯å¦è¯»å–å°¾éƒ¨
            file_size = os.path.getsize(filename)
            if file_size > 32768:  # è¶…è¿‡32KBæ‰è¯»å°¾éƒ¨
                f.seek(-16384, os.SEEK_END)
                tail = f.read(16384)
                hasher.update(tail)
                
            return (file_size, hasher.hexdigest())
    except Exception as e:
        print(f"å¿«é€Ÿå“ˆå¸Œå¤±è´¥[{filename}]: {str(e)}")
        return (None, None)

def get_full_hash(filename, expected_size):
    """å…¨æ–‡ä»¶å“ˆå¸Œæ ¡éªŒ"""
    try:
        # å…ˆéªŒè¯å½“å‰æ–‡ä»¶å¤§å°æ˜¯å¦åŒ¹é…
        actual_size = os.path.getsize(filename)
        if actual_size != expected_size:
            print(f"æ–‡ä»¶å¤§å°å˜åŒ–[{filename}]ï¼š{expected_size}=>{actual_size}")
            return None
    except OSError as e:
        print(f"æ— æ³•è·å–æ–‡ä»¶å¤§å°[{filename}]: {str(e)}")
        return None
    
    hasher = xxhash.xxh64()
    try:
        with open(filename, 'rb') as f:
            while True:
                chunk = f.read(8192)  # åˆ†å—è¯»å–é¿å…å¤§æ–‡ä»¶å†…å­˜é—®é¢˜
                if not chunk:
                    break
                hasher.update(chunk)
        return hasher.hexdigest()
    except Exception as e:
        print(f"å…¨å“ˆå¸Œè®¡ç®—å¤±è´¥[{filename}]: {str(e)}")
        return None

def scan_files(current_dir, recursive_mode):
    """æ™ºèƒ½æ–‡ä»¶æ‰«æ"""
    current_script = os.path.abspath(sys.argv[0])
    file_list = []
    
    # æ„å»ºæ’é™¤åˆ—è¡¨ï¼ˆé˜²æ­¢åˆ é™¤è„šæœ¬è‡ªèº«ï¼‰
    exclude_files = {current_script, os.path.abspath(__file__)}
    
    if recursive_mode:
        for root, dirs, files in os.walk(current_dir):
            for f in files:
                path = os.path.join(root, f)
                if path not in exclude_files:
                    file_list.append(path)
    else:
        file_list = [
            os.path.join(current_dir, f)
            for f in os.listdir(current_dir)
            if os.path.isfile(os.path.join(current_dir, f))
            and os.path.abspath(os.path.join(current_dir, f)) not in exclude_files
        ]
    
    return file_list

def process_group(group, global_auto_confirm, total_deleted, total_deleted_lock):
    """å®‰å…¨å¤„ç†é‡å¤æ–‡ä»¶ç»„"""
    size, _, full_hash, files = group
    current_dir = os.getcwd()
    sorted_files = sorted(files)
    to_keep = sorted_files[0]
    to_delete = sorted_files[1:]
    
    print(f"\nâ–Œé‡å¤æ–‡ä»¶ç»„ï¼ˆ{size}å­—èŠ‚ | å…¨å“ˆå¸Œ:{full_hash[:8]}...ï¼‰")
    print(f"ä¿ç•™: {os.path.relpath(to_keep, current_dir)}")
    print("å¾…åˆ é™¤:")
    for f in to_delete:
        print(f"  â”œ {os.path.relpath(f, current_dir)}")
    print("  â””â”€â”€â”€ç¡®è®¤åˆ é™¤ï¼Ÿâ”€â”€â”€")
    
    # ç”¨æˆ·ç¡®è®¤é€»è¾‘
    confirm = 'y' if global_auto_confirm.value else input("[Y]ç¡®è®¤/Nå–æ¶ˆ/YAå…¨éƒ¨ç¡®è®¤: ").strip().lower() or 'y'
    if confirm == 'ya':
        global_auto_confirm.value = True
        confirm = 'y'
    
    deleted_count = 0
    if confirm == 'y':
        for f in to_delete:
            try:
                os.remove(f)
                print(f"âœ“ å·²åˆ é™¤: {os.path.relpath(f, current_dir)}")
                with total_deleted_lock:
                    total_deleted.value += 1
                    deleted_count += 1
            except Exception as e:
                print(f"âœ• åˆ é™¤å¤±è´¥[{f}]: {str(e)}")
    
    return global_auto_confirm.value

def main():
    # å‚æ•°è§£æ
    recursive_mode = '-r' in sys.argv
    auto_confirm = '-y' in sys.argv
    
    # å¤šè¿›ç¨‹å…±äº«èµ„æº
    manager = Manager()
    total_deleted = manager.Value('i', 0)
    total_deleted_lock = manager.Lock()
    auto_confirm_flag = manager.Value('b', auto_confirm)
    
    # é˜¶æ®µ1ï¼šå¿«é€Ÿæ‰«æ
    print("ğŸ” æ‰«ææ–‡ä»¶ä¸­...")
    all_files = scan_files(os.getcwd(), recursive_mode)
    
    # é˜¶æ®µ2ï¼šå¿«é€Ÿå“ˆå¸Œåˆ†ç»„
    print("âš¡ å¿«é€Ÿå“ˆå¸Œé¢„å¤„ç†...")
    fast_hash_map = defaultdict(list)
    with ThreadPoolExecutor() as executor:
        futures = {executor.submit(get_fast_hash, f): f for f in all_files}
        for future in futures:
            path = futures[future]
            try:
                file_size, fast_hash = future.result()
                if file_size and fast_hash:
                    fast_hash_map[(file_size, fast_hash)].append(path)
            except Exception as e:
                print(f"å¤„ç†å¤±è´¥[{path}]: {str(e)}")
    
    # é˜¶æ®µ3ï¼šå…¨å“ˆå¸Œæ ¡éªŒ
    print("ğŸ”’ å…¨æ–‡ä»¶å“ˆå¸Œæ ¡éªŒ...")
    final_groups = []
    for (file_size, _), candidates in fast_hash_map.items():
        if len(candidates) < 2:
            continue
        
        # å¹¶è¡Œè®¡ç®—å…¨å“ˆå¸Œ
        full_hash_map = defaultdict(list)
        with ThreadPoolExecutor() as executor:
            futures = {executor.submit(get_full_hash, f, file_size): f for f in candidates}
            for future in futures:
                path = futures[future]
                full_hash = future.result()
                if full_hash:
                    full_hash_map[full_hash].append(path)
        
        # ç”Ÿæˆæœ€ç»ˆåˆ†ç»„
        for h, files in full_hash_map.items():
            if len(files) > 1:
                final_groups.append((file_size, None, h, files))
    
    # é˜¶æ®µ4ï¼šå¤„ç†é‡å¤æ–‡ä»¶
    print("\nğŸš€ å‘ç°", len(final_groups), "ä¸ªé‡å¤æ–‡ä»¶ç»„")
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for group in final_groups:
            futures.append(
                executor.submit(
                    process_group,
                    group,
                    auto_confirm_flag,
                    total_deleted,
                    total_deleted_lock
                )
            )
        for future in futures:
            future.result()
    
    print(f"\nâœ… å®Œæˆï¼å…±é‡Šæ”¾ {total_deleted.value} ä¸ªé‡å¤æ–‡ä»¶")

if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹ï¼špython dedup.py -r -y
    main()
